{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde30248-44f4-44e9-9387-cb949fe3e41f",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cc136-5842-46ec-b789-56973adb0f42",
   "metadata": {},
   "source": [
    "This notebook runs through the week 1 task from the MLX apprenticeship, namely building a language prediction model, and deploying it via Gradio and Tmux. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93f002-69d1-412f-8e90-86c507a70703",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26260f10-4209-4559-9fee-fb57e8533074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3258b-47c5-40cd-a602-d9238174bfee",
   "metadata": {},
   "source": [
    "# Solution Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207eb84-7f5a-4efd-9c28-61db5ba39711",
   "metadata": {},
   "source": [
    "- [Proposed dataset](https://huggingface.co/datasets/iix/Parquet_FIles)\n",
    "- Prep and tokenize dataset for word2vec model\n",
    "- Create word2vec model\n",
    "- Train word2vec model to get embedding matrix\n",
    "- Prep and tokenize dataset for MLP classification model\n",
    "- Create MLP classification model\n",
    "- Train MLP classification model\n",
    "- Launch persistent app via uvicorn/screen/tmux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71949b-781e-457b-9313-ed498604b10e",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd8a0a0-2219-4e2a-814e-106ed3b1e962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-12 08:54:29--  https://huggingface.co/datasets/iix/Parquet_FIles/resolve/main/Flores7Lang.parquet\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:2375:9e00:17:b174:6d00:93a1, 2600:9000:2375:2c00:17:b174:6d00:93a1, 2600:9000:2375:3e00:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:2375:9e00:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/fc/9a/fc9a918f6fbc19623c4d012eb54560ce331bef7d7208a8ec162d38c5b4a37971/ef205c73926e8ca0f0cb29276761c4716790d1e15498b0da361063c5348f5e9d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Flores7Lang.parquet%3B+filename%3D%22Flores7Lang.parquet%22%3B&Expires=1702630469&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjYzMDQ2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYy85YS9mYzlhOTE4ZjZmYmMxOTYyM2M0ZDAxMmViNTQ1NjBjZTMzMWJlZjdkNzIwOGE4ZWMxNjJkMzhjNWI0YTM3OTcxL2VmMjA1YzczOTI2ZThjYTBmMGNiMjkyNzY3NjFjNDcxNjc5MGQxZTE1NDk4YjBkYTM2MTA2M2M1MzQ4ZjVlOWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=dEcR3xno-469BQ0TImKOj40WKFJKwGBzH7MBPotRYbfuZ0fr%7EbRUzQJgguSSZOYwKMDsU4Op0ZXOK6pC1WdONKDNCUapv%7EapIsi4WOq4RC0YaV%7EybAedO7-pNxqDNCL3yyoHsdo85HtBTlqzlYEdq6tRo7FJkPdabvWIedlTsCbs%7EVc4Z0CeTLp01xJ3SomXBCOcuhx2exy0chUpa8h2E5lgf-MrF5OOfnOB8Qpdg%7ECDJL7K1Gou13lsfsf79hTtSlnJa%7Ex6bCyML1wvnWK5YwcDNswHMpdQnHuln4a-tbL%7EQLTMHH2DEdAPEWPpOyB826OXZl-h19yo3Nx44WYf1g__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-12-12 08:54:29--  https://cdn-lfs.huggingface.co/repos/fc/9a/fc9a918f6fbc19623c4d012eb54560ce331bef7d7208a8ec162d38c5b4a37971/ef205c73926e8ca0f0cb29276761c4716790d1e15498b0da361063c5348f5e9d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Flores7Lang.parquet%3B+filename%3D%22Flores7Lang.parquet%22%3B&Expires=1702630469&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjYzMDQ2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYy85YS9mYzlhOTE4ZjZmYmMxOTYyM2M0ZDAxMmViNTQ1NjBjZTMzMWJlZjdkNzIwOGE4ZWMxNjJkMzhjNWI0YTM3OTcxL2VmMjA1YzczOTI2ZThjYTBmMGNiMjkyNzY3NjFjNDcxNjc5MGQxZTE1NDk4YjBkYTM2MTA2M2M1MzQ4ZjVlOWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=dEcR3xno-469BQ0TImKOj40WKFJKwGBzH7MBPotRYbfuZ0fr%7EbRUzQJgguSSZOYwKMDsU4Op0ZXOK6pC1WdONKDNCUapv%7EapIsi4WOq4RC0YaV%7EybAedO7-pNxqDNCL3yyoHsdo85HtBTlqzlYEdq6tRo7FJkPdabvWIedlTsCbs%7EVc4Z0CeTLp01xJ3SomXBCOcuhx2exy0chUpa8h2E5lgf-MrF5OOfnOB8Qpdg%7ECDJL7K1Gou13lsfsf79hTtSlnJa%7Ex6bCyML1wvnWK5YwcDNswHMpdQnHuln4a-tbL%7EQLTMHH2DEdAPEWPpOyB826OXZl-h19yo3Nx44WYf1g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 2600:9000:2368:5000:11:f807:5180:93a1, 2600:9000:2368:5a00:11:f807:5180:93a1, 2600:9000:2368:9800:11:f807:5180:93a1, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:2368:5000:11:f807:5180:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1397523 (1.3M) [binary/octet-stream]\n",
      "Saving to: ‘Flores7Lang.parquet’\n",
      "\n",
      "Flores7Lang.parquet 100%[===================>]   1.33M  1.72MB/s    in 0.8s    \n",
      "\n",
      "2023-12-12 08:54:31 (1.72 MB/s) - ‘Flores7Lang.parquet’ saved [1397523/1397523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/iix/Parquet_FIles/resolve/main/Flores7Lang.parquet -O Flores7Lang.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503f97bc-0f20-40f0-9b18-a3959ae16eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"Flores7Lang.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b792c9-9f21-44c9-b72d-0123a69daf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe from wide to long format\n",
    "# Set value_vars to all columns, and no id_vars.\n",
    "# This creates a new row for every cell in the dataframe, labelled by the column name.\n",
    "data_long_format = data.melt(value_vars=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f6aaece-c691-43f0-a5db-6c3c7b8450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump all the translations into one unlabelled list\n",
    "# The W2V model doesn't need labelled data, that will come in the classification MLP\n",
    "w2v_corpus = data_long_format['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2866c176-a8a0-406c-a19c-44c5b58fbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise our W2V torch dataset class, with this corpus\n",
    "from lang_class_datasets import W2VData\n",
    "w2v_dataset = W2VData(w2v_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543987de-35ac-439f-a232-f5981ec72d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:corpus[0] Am Montag haben die Wisenschaftler der Stanford University School of Medicine die Erfindung eines neuen Diagnosetools bekanntgegeben, mit dem Zellen nach ihrem Typ sortiert werden können: ein winziger, ausdruckbarer Chip, der für jeweils etwa einen US-Cent mit Standard-Tintenstrahldruckern hergestellt werden kann.\n",
      "\n",
      "word2vec:ds[0] (tensor([12442,  4858,  9379, 14919]), tensor(7730))\n"
     ]
    }
   ],
   "source": [
    "# Examine some entries, to sanity check\n",
    "print(\"word2vec:corpus[0]\", w2v_corpus[0])\n",
    "print(\"word2vec:ds[0]\", w2v_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50663b6d-2dcb-427b-9922-974f3a28a89a",
   "metadata": {},
   "source": [
    "# Store Offline Tokenised Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a4970b-1793-48d5-a73b-3b3ed0cee2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26679\n",
      "['recuperato', 'faaliyetleri', 'buone', 'rendez', 'outdoors', 'diabete', 'minutes', 'spirituelle', 'broke', 'multaj']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "data = pd.read_parquet(\"./Flores7Lang.parquet\")\n",
    "long_format = data.melt(value_vars=data.columns)\n",
    "corpus = long_format[\"value\"].tolist()\n",
    "tknz = Tokenizer(corpus)\n",
    "tknz.save_vocab(\"./vocab.txt\")\n",
    "tknz.load_vocab(\"./vocab.txt\")\n",
    "print(len(tknz.vocab))\n",
    "print(tknz.vocab[90:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a94597-a884-491b-a6f2-bdf7f95e57a7",
   "metadata": {},
   "source": [
    "# Create DataLoader, CBOW Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e23798a-42b9-4dba-914c-67734819ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CBOW\n",
    "w2v_dataloader = torch.utils.data.DataLoader(w2v_dataset, batch_size=4, shuffle=True)\n",
    "# CBOW takes (vocab_size, embedding_dim) as inputs\n",
    "cbow = CBOW(len(w2v_dataset.tokenizer.vocab), 50)\n",
    "# Use negative log likelihood loss\n",
    "# In the CBOW task, we're examining the likelihood over the vocab set, in comparison to a ground truth (the missing word)\n",
    "# This is equivalent to a classification task. Guess the correct class amongst a set of classes\n",
    "# Hence, we use negative log likelihood\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdfd9c-7ac6-454f-be56-fc89656a4198",
   "metadata": {},
   "source": [
    "# Run Training Loop for W2V Model (Logging via Weights and Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8805f4-6f3f-4573-a78e-0c1cedc7b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  61%|██████▏   | 32047/52321 [03:55<02:28, 136.19batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(log_probs, target)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform backprop step (calc gradients of loss function wrt model params)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Change the parameter weights, according to selected optimizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in tqdm.tqdm(w2v_dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\", unit=\"batch\"):\n",
    "        # We don't want to accumulate gradients over batches, so zero them at the start of each batch's training loop\n",
    "        optimizer.zero_grad()\n",
    "        # PyTorch calls .forward() automatically, when we pass data to it\n",
    "        log_probs = cbow(context)\n",
    "        # Pass ground truth (target) and model outputs (log_probs) to loss function\n",
    "        loss = loss_function(log_probs, target)\n",
    "        # Perform backprop step (calc gradients of loss function wrt model params)\n",
    "        loss.backward()\n",
    "        # Change the parameter weights, according to selected optimizer\n",
    "        optimizer.step()\n",
    "        # Add up loss for printing purposes\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {total_loss}\")\n",
    "    torch.save(cbow.state_dict(), f\"./cbow_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf179199-1a5c-4c09-8ffc-aef98ba64c0d",
   "metadata": {},
   "source": [
    "# Prepare Data and DataLoader for Classification MLP Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf273aa4-bdc3-4bc2-861b-76a234adc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang_class_datasets import LangData\n",
    "# Need data in format (sentence, language_id, language)\n",
    "data = pd.read_parquet(\"./Flores7Lang.parquet\")\n",
    "lang_classification_ds = LangData(data)\n",
    "lang_classification_dl = torch.utils.data.DataLoader(lang_classification_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653044ec-16ee-46e6-bd0e-074b96f75b18",
   "metadata": {},
   "source": [
    "# Create Classification Model (initialized with W2V weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6547d927-b28e-420c-8981-88b2653b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Language\n",
    "vocab_size = len(lang_classification_ds.tknz.vocab)\n",
    "state_dict = torch.load(f'cbow_epoch_{n_epochs}.pt')\n",
    "pre_trained_cbow = CBOW(len(w2v_dataset.tokenizer.vocab), 50)\n",
    "pre_trained_cbow.load_state_dict(state_dict)\n",
    "classification_model = Language(pre_trained_cbow.embeddings.weight.data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7ce61-8e91-4dbd-a0e0-e57115f600ab",
   "metadata": {},
   "source": [
    "# Create Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fcb7fcc-147a-42de-af98-7316a7819400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification --> use cross entropy\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2964c7-51aa-4301-bf11-a222eeb8a1b2",
   "metadata": {},
   "source": [
    "# Run Training Loop for Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4f51f38-1ad5-48da-a39c-4197a32e1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1264.85batch/s]\n",
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1266.17batch/s]\n",
      "100%|████████████████████████████████| 14063/14063 [00:11<00:00, 1252.94batch/s]\n"
     ]
    }
   ],
   "source": [
    "n_epochs_classification = 3\n",
    "for epoch in range(n_epochs_classification):\n",
    "    for sentence, target, _ in tqdm.tqdm(lang_classification_dl, unit='batch'):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = classification_model(sentence)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Epoch {epoch+1}/5, Loss: {loss.item()}\")\n",
    "    torch.save(classification_model.state_dict(), f\"./classification_model_epoch_{n_epochs_classification+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759e90c-6910-4436-8db1-0fdd0c5210e4",
   "metadata": {},
   "source": [
    "# Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e70671ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26679, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Language\n",
    "from tokenizer import Tokenizer\n",
    "# Load pretrained classification model\n",
    "state_dict_classification = torch.load(f'classification_model_epoch_4.pt')\n",
    "# Initialise tokenizer with vocab\n",
    "tokenizer = (Tokenizer()).load_vocab(\"./vocab.txt\")\n",
    "# Use tokenizer vocab size to initialise language classification model\n",
    "classification_model = Language(torch.rand(len(tokenizer.vocab), 50), 7)\n",
    "print (pre_trained_cbow.embeddings.weight.data.shape)\n",
    "# Load pre-trained weights into classification model\n",
    "classification_model.load_state_dict(state_dict_classification)\n",
    "# Set language ordering\n",
    "langs = [\"German\", \"Esperanto\", \"French\", \"Italian\", \"Spanish\", \"Turkish\", \"English\"]\n",
    "classification_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d2714",
   "metadata": {},
   "source": [
    "# Predict language of dummy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c1b72ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class': 'German', 'value': 0.0076903305016458035},\n",
       " {'class': 'Esperanto', 'value': 0.34975698590278625},\n",
       " {'class': 'French', 'value': 0.1005527600646019},\n",
       " {'class': 'Italian', 'value': 0.020299967378377914},\n",
       " {'class': 'Spanish', 'value': 0.44341129064559937},\n",
       " {'class': 'Turkish', 'value': 0.060587987303733826},\n",
       " {'class': 'English', 'value': 0.01770068146288395}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'the cat sat on the hat'\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "# Pass tokens through forward method of model\n",
    "predictions = classification_model(tokens)\n",
    "# Softmax to get probability distribution\n",
    "predictions_softmaxed = torch.nn.functional.softmax(predictions, dim=1)\n",
    "predictions_softmaxed = predictions_softmaxed.squeeze(0).tolist()\n",
    "\n",
    "result = [{\"class\": class_name, \"value\": value} for class_name, value in zip(langs, predictions_softmaxed)]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02a11d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
